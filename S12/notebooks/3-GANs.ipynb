{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "[Ref](https://github.com/kroosen/GAN-in-keras-on-mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dimensions of the noise\n",
    "z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "g = Sequential()\n",
    "g.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))\n",
    "g.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "g.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\n",
    "g.add(Dense(784, activation='sigmoid'))  # Values between 0 and 1\n",
    "g.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "d = Sequential()\n",
    "d.add(Dense(1024, input_dim=784, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "d.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "d.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dropout(0.3))\n",
    "d.add(Dense(1, activation='sigmoid'))  # Values between 0 and 1\n",
    "d.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "d.trainable = False\n",
    "inputs = Input(shape=(z_dim, ))\n",
    "hidden = g(inputs)\n",
    "output = d(hidden)\n",
    "gan = Model(inputs, output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    \"\"\"\n",
    "    @losses.keys():\n",
    "        0: loss\n",
    "        1: accuracy\n",
    "    \"\"\"\n",
    "    d_loss = [v[0] for v in losses[\"D\"]]\n",
    "    g_loss = [v[0] for v in losses[\"G\"]]\n",
    "    #d_acc = [v[1] for v in losses[\"D\"]]\n",
    "    #g_acc = [v[1] for v in losses[\"G\"]]\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
    "    plt.plot(g_loss, label=\"Generator loss\")\n",
    "    #plt.plot(d_acc, label=\"Discriminator accuracy\")\n",
    "    #plt.plot(g_acc, label=\"Generator accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_generated(n_ex=10, dim=(1, 10), figsize=(12, 2)):\n",
    "    noise = np.random.normal(0, 1, size=(n_ex, z_dim))\n",
    "    generated_images = g.predict(noise)\n",
    "    generated_images = generated_images.reshape(n_ex, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a vector (dict) to store the losses\n",
    "losses = {\"D\":[], \"G\":[]}\n",
    "\n",
    "def train(epochs=1, plt_frq=1, BATCH_SIZE=128):\n",
    "    batchCount = int(X_train.shape[0] / BATCH_SIZE)\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', BATCH_SIZE)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "    \n",
    "    for e in tqdm_notebook(range(1, epochs+1)):\n",
    "        if e == 1 or e%plt_frq == 0:\n",
    "            print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in range(batchCount):  # tqdm_notebook(range(batchCount), leave=False):\n",
    "            # Create a batch by drawing random index numbers from the training set\n",
    "            image_batch = X_train[np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)]\n",
    "            # Create noise vectors for the generator\n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
    "            \n",
    "            # Generate the images from the noise\n",
    "            generated_images = g.predict(noise)\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            # Create labels\n",
    "            y = np.zeros(2*BATCH_SIZE)\n",
    "            y[:BATCH_SIZE] = 0.9  # One-sided label smoothing\n",
    "\n",
    "            # Train discriminator on generated images\n",
    "            d.trainable = True\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
    "            y2 = np.ones(BATCH_SIZE)\n",
    "            d.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y2)\n",
    "\n",
    "        # Only store losses from final batch of epoch\n",
    "        losses[\"D\"].append(d_loss)\n",
    "        losses[\"G\"].append(g_loss)\n",
    "\n",
    "        # Update the plots\n",
    "        if e == 1 or e%plt_frq == 0:\n",
    "            plot_generated()\n",
    "    plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=200, plt_frq=20, BATCH_SIZE=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D, Convolution2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dimensions of the noise\n",
    "z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nch = 200\n",
    "g_input = Input(shape=[100])\n",
    "H = Dense(nch*14*14, init='glorot_normal')(g_input)\n",
    "H = BatchNormalization()(H)\n",
    "H = Activation('relu')(H)\n",
    "H = Reshape( [nch, 14, 14] )(H)\n",
    "H = UpSampling2D(size=(2, 2))(H)\n",
    "H = Convolution2D(int(nch/2), 3, 3, border_mode='same', init='glorot_uniform')(H)\n",
    "H = BatchNormalization()(H)\n",
    "H = Activation('relu')(H)\n",
    "H = Convolution2D(int(nch/4), 3, 3, border_mode='same', init='glorot_uniform')(H)\n",
    "H = BatchNormalization()(H)\n",
    "H = Activation('relu')(H)\n",
    "H = Convolution2D(1, 1, 1, border_mode='same', init='glorot_uniform')(H)\n",
    "g_V = Activation('sigmoid')(H)\n",
    "generator = Model(g_input,g_V)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "g = Sequential()\n",
    "g.add(Dense(7*7*112, input_dim=z_dim))\n",
    "g.add(Reshape((7, 7, 112)))\n",
    "g.add(BatchNormalization())\n",
    "g.add(Activation(LeakyReLU(alpha=0.2)))\n",
    "g.add(Conv2DTranspose(56, 5, strides=2, padding='same'))\n",
    "g.add(BatchNormalization())\n",
    "g.add(Activation(LeakyReLU(alpha=0.2)))\n",
    "g.add(Conv2DTranspose(1, 5, strides=2, padding='same', activation='sigmoid'))\n",
    "g.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "g.summary()\n",
    "\n",
    "d = Sequential()\n",
    "d.add(Conv2D(56, 5, strides=2, padding='same', input_shape=(28, 28, 1), activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Conv2D(112, 5, strides=2, padding='same'))\n",
    "g.add(BatchNormalization())\n",
    "g.add(Activation(LeakyReLU(alpha=0.2)))\n",
    "d.add(Conv2D(224, 5, strides=2, padding='same'))\n",
    "g.add(Activation(LeakyReLU(alpha=0.2)))\n",
    "d.add(Flatten())\n",
    "d.add(Dense(112, activation=LeakyReLU(alpha=0.2)))\n",
    "d.add(Dense(1, activation='sigmoid'))\n",
    "d.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "d.summary()\n",
    "\n",
    "d.trainable = False\n",
    "inputs = Input(shape=(z_dim, ))\n",
    "hidden = g(inputs)\n",
    "output = d(hidden)\n",
    "gan = Model(inputs, output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    \"\"\"\n",
    "    @losses.keys():\n",
    "        0: loss\n",
    "        1: accuracy\n",
    "    \"\"\"\n",
    "    d_loss = [v[0] for v in losses[\"D\"]]\n",
    "    g_loss = [v[0] for v in losses[\"G\"]]\n",
    "    \n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
    "    plt.plot(g_loss, label=\"Generator loss\")\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_generated(n_ex=10, dim=(1, 10), figsize=(12, 2)):\n",
    "    noise = np.random.normal(0, 1, size=(n_ex, z_dim))\n",
    "    generated_images = g.predict(noise)\n",
    "    generated_images = generated_images.reshape(generated_images.shape[0], 28, 28)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i, :, :], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a vector (dict) to store the losses\n",
    "losses = {\"D\":[], \"G\":[]}\n",
    "samples = []\n",
    "\n",
    "def train(epochs=1, plt_frq=1, BATCH_SIZE=128):\n",
    "    batchCount = int(X_train.shape[0] / BATCH_SIZE)\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', BATCH_SIZE)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "    \n",
    "    for e in tqdm_notebook(range(1, epochs+1)):\n",
    "        if e == 1 or e%plt_frq == 0:\n",
    "            print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in range(batchCount):  # tqdm_notebook(range(batchCount), leave=False):\n",
    "            # Create a batch by drawing random index numbers from the training set\n",
    "            image_batch = X_train[np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)]\n",
    "            image_batch = image_batch.reshape(image_batch.shape[0], image_batch.shape[1], image_batch.shape[2], 1)\n",
    "            # Create noise vectors for the generator\n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
    "            \n",
    "            # Generate the images from the noise\n",
    "            generated_images = g.predict(noise)\n",
    "            samples.append(generated_images)\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            # Create labels\n",
    "            y = np.zeros(2*BATCH_SIZE)\n",
    "            y[:BATCH_SIZE] = 0.9  # One-sided label smoothing\n",
    "\n",
    "            # Train discriminator on generated images\n",
    "            d.trainable = True\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\n",
    "            y2 = np.ones(BATCH_SIZE)\n",
    "            d.trainable = False\n",
    "            g_loss = gan.train_on_batch(noise, y2)\n",
    "\n",
    "        # Only store losses from final batch of epoch\n",
    "        losses[\"D\"].append(d_loss)\n",
    "        losses[\"G\"].append(g_loss)\n",
    "\n",
    "        # Update the plots\n",
    "        if e == 1 or e%plt_frq == 0:\n",
    "            plot_generated()\n",
    "    plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=200, plt_frq=20, BATCH_SIZE=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
